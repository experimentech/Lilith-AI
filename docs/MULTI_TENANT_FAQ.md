# Multi-Tenant Architecture FAQ

## Question 1: BioNN State Sharing Across Users

**Q: What happens with the state of the BNNs on each layer when there are multiple users? Will it affect them negatively or will they potentially learn more quickly?**

### Current State: BNNs are Stateless Encoders

**Short Answer**: The BNNs don't have user-specific state - they're **read-only encoders** that generate embeddings. All learning happens through **success score updates** in the pattern database, which ARE isolated per user.

**How It Works**:

1. **BioNN Encoder** (`PMFlowEmbeddingEncoder`):
   - **Stateless**: Same weights for all users
   - **Role**: Converts text ‚Üí embedding vectors
   - **No Learning**: Weights are frozen after training
   - **Shared Benefit**: All users get the same semantic understanding

2. **Pattern Learning** (Per-User):
   - **User-Specific Database**: Each user has isolated SQLite database
   - **Success Scores**: Updated based on conversation feedback
   - **Pattern Selection**: Higher success scores ‚Üí more likely to be retrieved
   - **Learning Mechanism**: Reinforcement through success score adjustment

### Multi-User Learning Dynamics

**Currently**: 
- ‚úÖ Users **share** the BioNN encoder (semantic understanding)
- ‚úÖ Users **don't share** learned patterns (isolation)
- ‚úÖ Teacher can update **base knowledge** (shared facts)
- ‚ùå Users **can't benefit** from each other's learning

**Example Scenario**:
```
Teacher adds: "Paris is the capital of France" (base knowledge)
Alice asks: "What's the capital of France?"
  ‚Üí Retrieves from base: "Paris is the capital of France" ‚úì

Bob asks: "capital of France?"
  ‚Üí Also retrieves from base: "Paris is the capital of France" ‚úì
  ‚Üí Both users benefit from teacher's knowledge

Alice learns: "My favorite color is blue" (personal)
Bob asks: "What's Alice's favorite color?"
  ‚Üí Bob's database has no knowledge of Alice's preferences ‚úó
  ‚Üí Proper isolation maintained
```

### Future Enhancement: Federated Learning

**Potential Improvement** (not yet implemented):

```python
class FederatedBNNLearner:
    """
    Aggregate learning across users while maintaining privacy.
    
    - Users contribute gradient updates (not raw data)
    - BioNN encoder improves from collective experience
    - Better semantic understanding over time
    - Privacy preserved through differential privacy
    """
```

**Benefits**:
- üöÄ Faster learning from collective experience
- üß† BioNN gets better at understanding user language patterns
- üîí Privacy maintained (only gradients shared, not personal data)
- üìà New users benefit from existing knowledge

**Tradeoffs**:
- ‚öôÔ∏è More complex implementation
- üîÑ Need gradient aggregation infrastructure
- ‚è±Ô∏è Periodic BioNN weight updates required

## Question 2: .gitignore Configuration

**Q: Has .gitignore been updated so it doesn't include the test users?**

### Answer: Yes, Updated

The `.gitignore` has been updated to exclude:

```gitignore
# SQLite databases (all user data)
*.db
*.db-journal
*.sqlite
*.sqlite3

# User data directories (explicit exclusion)
data/users/        # All user databases
data/test/         # Test data
data/demo/         # Demo data

# Pattern stores (JSON legacy)
*_patterns.json
response_patterns.json
```

### What Gets Tracked:

‚úÖ **Code**: All Python files in `lilith/`
‚úÖ **Tests**: Test scripts (not test data)
‚úÖ **Base Knowledge**: Can optionally track `data/base/response_patterns.db` if needed
‚úÖ **Documentation**: All markdown files

‚ùå **Not Tracked**:
- User databases (`data/users/*/response_patterns.db`)
- Test data (`data/test/`)
- Demo data (`data/demo/`)
- Any `*.db` files by default

### Base Knowledge Tracking (Optional):

If you want to version control the shared base knowledge:

```bash
# Force add base knowledge to git
git add -f data/base/response_patterns.db

# Or update .gitignore to allow it:
# Change: *.db
# To:     
*.db
!data/base/response_patterns.db  # Allow base knowledge
```

## Question 3: User Data Separation & Maintenance

**Q: Are the individual user's data separated to allow for easy maintenance such as deletion of inactive users?**

### Answer: Yes, Fully Separated

Each user has a completely isolated database file:

```
data/
‚îú‚îÄ‚îÄ base/
‚îÇ   ‚îî‚îÄ‚îÄ response_patterns.db          # Shared base knowledge
‚îî‚îÄ‚îÄ users/
    ‚îú‚îÄ‚îÄ alice/
    ‚îÇ   ‚îî‚îÄ‚îÄ response_patterns.db      # Alice's personal patterns
    ‚îú‚îÄ‚îÄ bob/
    ‚îÇ   ‚îî‚îÄ‚îÄ response_patterns.db      # Bob's personal patterns
    ‚îî‚îÄ‚îÄ charlie/
        ‚îî‚îÄ‚îÄ response_patterns.db      # Charlie's personal patterns
```

### User Maintenance Operations

#### 1. Delete Inactive User

```bash
# Simple deletion - remove user's directory
rm -rf data/users/alice/

# That's it! User completely removed
```

#### 2. Archive User Data

```bash
# Archive before deletion
tar -czf alice_backup_2025-11-26.tar.gz data/users/alice/
mv alice_backup_2025-11-26.tar.gz archives/
rm -rf data/users/alice/
```

#### 3. List All Users

```bash
# See all registered users
ls data/users/
```

#### 4. Check User Database Size

```bash
# Find large user databases
du -h data/users/*/response_patterns.db | sort -h
```

#### 5. Export User Data

```python
# Python script to export user patterns
from lilith.response_fragments_sqlite import ResponseFragmentStoreSQLite
from lilith.embedding import PMFlowEmbeddingEncoder

encoder = PMFlowEmbeddingEncoder(dimension=64, latent_dim=32)
store = ResponseFragmentStoreSQLite(
    encoder,
    storage_path="data/users/alice/response_patterns.db",
    bootstrap_if_empty=False
)

# Export to JSON for portability
import json
patterns = store.patterns  # Get all patterns
with open("alice_export.json", "w") as f:
    json.dump(patterns, f, indent=2)
```

### Automated Maintenance Script

Create `tools/manage_users.py`:

```python
#!/usr/bin/env python3
"""User database maintenance utility."""

import sys
from pathlib import Path
import shutil
from datetime import datetime

def list_users():
    """List all users and database sizes."""
    users_dir = Path("data/users")
    if not users_dir.exists():
        print("No users directory found")
        return
    
    print("User Databases:")
    print("-" * 60)
    
    total_size = 0
    for user_dir in sorted(users_dir.iterdir()):
        if user_dir.is_dir():
            db_path = user_dir / "response_patterns.db"
            if db_path.exists():
                size = db_path.stat().st_size
                total_size += size
                size_mb = size / (1024 * 1024)
                print(f"{user_dir.name:20s} {size_mb:>10.2f} MB")
    
    print("-" * 60)
    print(f"Total: {total_size / (1024 * 1024):.2f} MB")

def delete_user(user_id: str, archive: bool = True):
    """Delete user data with optional archival."""
    user_dir = Path(f"data/users/{user_id}")
    
    if not user_dir.exists():
        print(f"User {user_id} not found")
        return
    
    if archive:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        archive_path = Path(f"archives/users/{user_id}_{timestamp}.tar.gz")
        archive_path.parent.mkdir(parents=True, exist_ok=True)
        
        import tarfile
        with tarfile.open(archive_path, "w:gz") as tar:
            tar.add(user_dir, arcname=user_id)
        
        print(f"‚úÖ Archived to {archive_path}")
    
    shutil.rmtree(user_dir)
    print(f"‚úÖ Deleted user: {user_id}")

def cleanup_inactive(days_inactive: int = 90):
    """Delete users inactive for N days."""
    import time
    from sqlite3 import connect
    
    cutoff = time.time() - (days_inactive * 86400)
    users_dir = Path("data/users")
    
    for user_dir in users_dir.iterdir():
        if not user_dir.is_dir():
            continue
        
        db_path = user_dir / "response_patterns.db"
        if not db_path.exists():
            continue
        
        # Check last updated timestamp from database
        conn = connect(str(db_path))
        cursor = conn.execute(
            "SELECT MAX(updated_at) FROM response_patterns"
        )
        last_update = cursor.fetchone()[0]
        conn.close()
        
        if last_update:
            # Parse timestamp and check if inactive
            # (Implementation depends on timestamp format)
            pass

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("Usage: python manage_users.py [list|delete|cleanup]")
        sys.exit(1)
    
    command = sys.argv[1]
    
    if command == "list":
        list_users()
    elif command == "delete" and len(sys.argv) > 2:
        user_id = sys.argv[2]
        delete_user(user_id)
    elif command == "cleanup":
        days = int(sys.argv[2]) if len(sys.argv) > 2 else 90
        cleanup_inactive(days)
```

## Question 4: Correcting Bad Information

**Q: What happens if Lilith learns some bad information? For example "an apple is a banana", "a banana is a vegetable", or that sort of thing? Can the incorrect associations decay and be overridden by correct assertions or implied information in conversation?**

### Answer: Yes, Through Success Score Decay

### Current Mechanism: Reinforcement Learning

The system uses **success scores** that can be updated based on conversation outcomes:

```python
def update_success(self, fragment_id: str, outcome: bool, learning_rate: float = 0.1):
    """
    Update pattern success score based on outcome.
    
    Args:
        fragment_id: Pattern to update
        outcome: True if successful, False if failed
        learning_rate: How fast to learn (0.1 = 10% adjustment)
    """
    if outcome:
        # Reinforce: Move toward 1.0
        new_score = current_score + learning_rate * (1.0 - current_score)
    else:
        # Decay: Move toward 0.0
        new_score = current_score - learning_rate * current_score
```

### Example: Correcting "Apple is a Banana"

**Scenario 1: Natural Decay from Negative Feedback**

```
User teaches (bad info): "An apple is a type of banana"
  ‚Üí Lilith stores: success_score = 0.5

User: "What is an apple?"
Lilith: "An apple is a type of banana"
User: "No, that's wrong"
  ‚Üí update_success(pattern_id, outcome=False)
  ‚Üí success_score: 0.5 ‚Üí 0.45 ‚Üí 0.40 ‚Üí 0.35...

User teaches (correct): "An apple is a fruit that grows on trees"
  ‚Üí Lilith stores: success_score = 0.5

User: "What is an apple?"
Lilith retrieves both patterns, ranks by success_score:
  1. "fruit that grows on trees" (0.5) ‚Üê Higher score
  2. "type of banana" (0.35)         ‚Üê Lower score
  ‚Üí Returns correct answer

After more conversations with negative feedback:
  ‚Üí Wrong pattern decays: 0.35 ‚Üí 0.30 ‚Üí 0.20 ‚Üí 0.10
  ‚Üí Correct pattern reinforced: 0.5 ‚Üí 0.6 ‚Üí 0.7 ‚Üí 0.8
  ‚Üí Wrong pattern eventually ignored (score too low)
```

**Decay Over Time**:
```
Initial:  success_score = 0.5
After 1 negative: 0.45
After 2 negative: 0.405
After 3 negative: 0.3645
After 5 negative: 0.295
After 10 negative: 0.174
After 20 negative: 0.030  ‚Üê Essentially dead
```

### Scenario 2: Teacher Override

```python
# Teacher mode can directly update base knowledge
teacher_store.add_pattern(
    trigger_context="what is an apple",
    response_text="An apple is a fruit that grows on apple trees",
    success_score=0.95,  # High confidence
    intent="biology"
)

# This creates strong base knowledge that users retrieve
# User's wrong pattern (0.35) vs base correct pattern (0.95)
# ‚Üí Base pattern wins every time
```

### Enhancements for Better Correction

#### 1. Explicit Negative Learning

```python
def correct_misinformation(self, wrong_pattern_id: str, correct_pattern_id: str):
    """Explicitly mark pattern as wrong and reinforce correct one."""
    # Severely penalize wrong pattern
    self.update_success(wrong_pattern_id, outcome=False, learning_rate=0.5)
    
    # Strongly reinforce correct pattern
    self.update_success(correct_pattern_id, outcome=True, learning_rate=0.3)
```

#### 2. Pattern Pruning

```python
def prune_low_quality_patterns(self, threshold: float = 0.1):
    """Remove patterns with very low success scores."""
    conn = self._get_connection()
    
    # Delete patterns below threshold
    conn.execute(
        "DELETE FROM response_patterns WHERE success_score < ?",
        (threshold,)
    )
    conn.commit()
    conn.close()
    
    print(f"Pruned patterns with success_score < {threshold}")
```

#### 3. Contradiction Detection

```python
def detect_contradictions(self, patterns: List[ResponsePattern]) -> List[Tuple]:
    """
    Detect contradictory patterns using semantic similarity.
    
    Example:
      Pattern A: "An apple is a banana"
      Pattern B: "An apple is a fruit"
      ‚Üí High context similarity, different responses ‚Üí Contradiction
    """
    contradictions = []
    
    for i, p1 in enumerate(patterns):
        for p2 in patterns[i+1:]:
            # Same context (similar trigger)?
            context_sim = self._compute_similarity(
                p1.trigger_context, 
                p2.trigger_context
            )
            
            # Different response?
            response_sim = self._compute_similarity(
                p1.response_text, 
                p2.response_text
            )
            
            if context_sim > 0.8 and response_sim < 0.3:
                # Flag contradiction
                contradictions.append((p1, p2))
    
    return contradictions
```

#### 4. Time-Based Decay

```python
def apply_temporal_decay(self, half_life_days: int = 30):
    """
    Decay unused patterns over time.
    
    Patterns not used recently slowly lose confidence.
    """
    conn = self._get_connection()
    
    # Decay based on last updated time
    conn.execute("""
        UPDATE response_patterns
        SET success_score = success_score * 
            (0.5 ^ (julianday('now') - julianday(updated_at)) / ?)
        WHERE julianday('now') - julianday(updated_at) > ?
    """, (half_life_days, half_life_days))
    
    conn.commit()
    conn.close()
```

### Recommended Workflow for Quality Control

1. **User Level**: 
   - Automatic success score updates from feedback
   - Natural decay of wrong information
   - User can't corrupt base knowledge

2. **Teacher Level**:
   - Regular review of base patterns
   - Add high-confidence correct patterns (score 0.9+)
   - Prune contradictions periodically

3. **System Level**:
   - Run `prune_low_quality_patterns()` weekly
   - Apply `temporal_decay()` monthly
   - Monitor for contradictions

### Example Maintenance Script

```python
#!/usr/bin/env python3
"""Quality control for Lilith knowledge base."""

from lilith.response_fragments_sqlite import ResponseFragmentStoreSQLite
from lilith.embedding import PMFlowEmbeddingEncoder

def quality_control():
    encoder = PMFlowEmbeddingEncoder(dimension=64, latent_dim=32)
    
    # Base knowledge maintenance
    base_store = ResponseFragmentStoreSQLite(
        encoder,
        storage_path="data/base/response_patterns.db"
    )
    
    print("üßπ Running quality control...")
    
    # 1. Prune low-quality patterns
    pruned = base_store.prune_low_quality_patterns(threshold=0.15)
    print(f"‚úÖ Pruned {pruned} low-quality patterns")
    
    # 2. Apply temporal decay
    base_store.apply_temporal_decay(half_life_days=60)
    print(f"‚úÖ Applied temporal decay")
    
    # 3. Detect contradictions
    patterns = base_store.patterns
    contradictions = base_store.detect_contradictions(patterns)
    if contradictions:
        print(f"‚ö†Ô∏è  Found {len(contradictions)} contradictions:")
        for p1, p2 in contradictions[:5]:  # Show first 5
            print(f"  - '{p1.trigger_context}' ‚Üí '{p1.response_text}'")
            print(f"    vs '{p2.response_text}' (scores: {p1.success_score:.2f} vs {p2.success_score:.2f})")

if __name__ == "__main__":
    quality_control()
```

## Summary

1. **BioNN State**: Shared encoder (stateless), isolated pattern learning ‚úì
2. **Git Ignore**: Updated to exclude all user data ‚úì
3. **User Separation**: Complete isolation, easy maintenance ‚úì
4. **Bad Information**: Natural decay + teacher override + pruning tools ‚úì
5. **Corruption Recovery**: Automatic detection and recovery ‚úì
6. **User Data Reset**: Clean slate with backup ‚úì
7. **Manual Feedback**: Upvote/downvote for quality control ‚úì

The architecture is designed for robust multi-user operation with built-in mechanisms for knowledge quality control.

---

## Question 5: Database Corruption Handling

**Q: What happens if a user's data is corrupted? Will it crash Lilith, or will it gracefully handle it?**

### Answer: Graceful Recovery with Automatic Backup

Lilith **detects and recovers** from database corruption automatically:

#### Corruption Detection

On database initialization, Lilith runs integrity checks:

```python
# Automatic integrity check
cursor = conn.execute("PRAGMA integrity_check")
integrity = cursor.fetchone()[0]
if integrity != "ok":
    raise sqlite3.DatabaseError(f"Database corruption detected")
```

#### Recovery Process

When corruption is detected:

1. **Backup**: Corrupted database is backed up with timestamp
   ```
   response_patterns.corrupted.20251126_083351.db
   ```

2. **Data Recovery**: Attempts to salvage recoverable patterns
   ```python
   # Try to read what we can
   for row in cursor:
       try:
           recoverable_patterns.append(dict(row))
       except:
           pass  # Skip corrupted rows
   ```

3. **Fresh Database**: Creates new clean database

4. **Restore**: Re-inserts recovered patterns

5. **Report**: Shows what was saved
   ```
   ‚úÖ Database recovered successfully
   üìù Restored 27/30 patterns
   üíæ Backup at: response_patterns.corrupted.20251126_083351.db
   ```

#### User Experience

**What the user sees:**
```
‚ö†Ô∏è  Database corruption detected
‚ö†Ô∏è  Attempting to recover database: data/users/alice/response_patterns.db
üíæ Backed up corrupted database
‚úÖ Recovered 27 patterns
‚úÖ Database recovered successfully
```

**System continues running** - no crash!

#### If Recovery Fails

If automatic recovery fails:
```
‚ùå Recovery failed: [error details]
üí° Manual intervention required
üíæ Backup available at: [path]
```

User's original data is **always backed up** before any recovery attempt.

---

## Question 6: User Data Reset

**Q: Provide an option for users to reset their user data. For example if they want a clean slate for whatever reason.**

### Answer: Yes - `reset_user_data()` Method

Users can completely reset their data with automatic backup:

#### Usage

```python
from lilith.multi_tenant_store import MultiTenantFragmentStore
from lilith.user_auth import UserIdentity, AuthMode
from lilith.embedding import PMFlowEmbeddingEncoder

encoder = PMFlowEmbeddingEncoder(dimension=64, latent_dim=32)
user = UserIdentity("alice", AuthMode.SIMPLE, "Alice")
store = MultiTenantFragmentStore(encoder, user)

# Reset to clean slate (with backup)
backup_path = store.reset_user_data(keep_backup=True)
```

**Output:**
```
üíæ Backup created: data/users/alice/response_patterns.backup.20251126_083353.db
üîÑ Database reset complete
üìä Old patterns: 47
üìä New patterns: 0
üîÑ Reset complete for Alice
```

#### Options

```python
# Reset without backup (dangerous!)
store.reset_user_data(keep_backup=False)

# Reset with seed patterns
store.reset_user_data(bootstrap=True)  # Starts with 4 seed patterns
```

#### Command-Line Interface

Add to `lilith_cli.py`:

```python
def reset_command(args):
    """Reset user data command."""
    encoder = PMFlowEmbeddingEncoder(dimension=64, latent_dim=32)
    user = UserIdentity(args.user, AuthMode.SIMPLE, args.user.title())
    store = MultiTenantFragmentStore(encoder, user)
    
    # Confirm
    print(f"‚ö†Ô∏è  This will delete all data for user: {args.user}")
    print("A backup will be created.")
    confirm = input("Continue? (yes/no): ")
    
    if confirm.lower() == "yes":
        backup = store.reset_user_data(keep_backup=True)
        print(f"‚úÖ Reset complete. Backup at: {backup}")
    else:
        print("‚ùå Reset cancelled")

# Usage:
# python lilith_cli.py reset --user alice
```

#### Teacher Mode Protection

Teachers **cannot** reset user data in teacher mode:

```python
teacher_store.reset_user_data()
# PermissionError: Cannot reset user data in teacher mode
```

To reset **base knowledge** (careful!):
```python
teacher_store.base_store.reset_database(keep_backup=True, bootstrap=True)
```

---

## Question 7: Manual Reward/Punishment

**Q: Manual reward/punishment commands for replies. Upvote/downvote, helpful/not helpful. That sort of thing. So the user can intervene if a particularly good or bad answer is provided.**

### Answer: Yes - Four Feedback Methods

Lilith provides **manual feedback mechanisms** with different strengths:

### 1. Upvote (Strong Positive)

For **particularly good** responses:

```python
# After getting a great response
pattern_id = "pattern_geography_1764106433745_2623"
store.upvote(pattern_id, strength=0.2)
```

**Effect:**
- Success score moves 20% toward 1.0
- Pattern more likely to be retrieved in future
- `0.50 ‚Üí 0.60` (one upvote)

**Strength options:**
```python
store.upvote(pattern_id, strength=0.1)  # Mild boost
store.upvote(pattern_id, strength=0.2)  # Default (moderate)
store.upvote(pattern_id, strength=0.4)  # Strong boost
```

### 2. Downvote (Strong Negative)

For **incorrect or unhelpful** responses:

```python
# After getting a bad response
store.downvote(pattern_id, strength=0.3)
```

**Effect:**
- Success score moves 30% toward 0.0
- Pattern less likely to be retrieved
- `0.60 ‚Üí 0.42` (one downvote)

**Strength options:**
```python
store.downvote(pattern_id, strength=0.2)  # Moderate penalty
store.downvote(pattern_id, strength=0.3)  # Default
store.downvote(pattern_id, strength=0.5)  # Severe penalty
```

### 3. Mark Helpful (Moderate Positive)

For **generally good** responses:

```python
store.mark_helpful(pattern_id)
```

**Effect:**
- 15% boost toward 1.0
- Lighter than upvote
- Good for routine positive feedback

### 4. Mark Not Helpful (Moderate Negative)

For **suboptimal** responses:

```python
store.mark_not_helpful(pattern_id)
```

**Effect:**
- 15% penalty toward 0.0
- Lighter than downvote
- Good for responses that aren't quite right

### Comparison of Feedback Strengths

| Method | Strength | Use Case |
|--------|----------|----------|
| `upvote()` | 20% | Excellent response - want to see more like this |
| `mark_helpful()` | 15% | Good response - generally on track |
| `mark_not_helpful()` | 15% | Suboptimal - needs improvement |
| `downvote()` | 30% | Bad response - actively harmful/wrong |

### Interactive Conversation Loop

Example integration:

```python
def conversation_with_feedback(store):
    """Conversation loop with manual feedback."""
    
    while True:
        # Get user input
        user_input = input("You: ")
        if user_input.lower() in ['quit', 'exit']:
            break
        
        # Retrieve response
        patterns = store.retrieve_patterns(user_input, topk=1)
        
        if patterns:
            pattern, confidence = patterns[0]
            print(f"Lilith: {pattern.response_text}")
            print(f"  (confidence: {confidence:.2f}, score: {pattern.success_score:.2f})")
            
            # Manual feedback prompt
            feedback = input("Feedback [+1=upvote, +0.5=helpful, -0.5=not helpful, -1=downvote, Enter=skip]: ")
            
            if feedback == "+1":
                store.upvote(pattern.fragment_id)
            elif feedback == "+0.5":
                store.mark_helpful(pattern.fragment_id)
            elif feedback == "-0.5":
                store.mark_not_helpful(pattern.fragment_id)
            elif feedback == "-1":
                store.downvote(pattern.fragment_id)
            
        else:
            print("Lilith: I'm not sure how to respond to that.")
```

### Multi-Tenant Feedback

**User can feedback on:**
- ‚úÖ Their own patterns (full control)
- ‚ùå Base patterns (protected - warning issued)

**Teacher can feedback on:**
- ‚úÖ All base patterns
- ‚úÖ Their own patterns (if in teacher mode)

**Example:**
```python
# User tries to downvote base knowledge
user_store.downvote(base_pattern_id)
# ‚ö†Ô∏è  Cannot modify base pattern in user mode: pattern_math_1764106434383_3998

# Teacher CAN modify base patterns
teacher_store.downvote(base_pattern_id)
# üëé Downvoted pattern: pattern_math_1764106434383_3998
```

### Feedback History

Success scores persist across sessions:

```python
# Session 1
store.upvote(pattern_id)  # 0.50 ‚Üí 0.60

# Session 2 (later)
store.upvote(pattern_id)  # 0.60 ‚Üí 0.68

# Session 3
store.downvote(pattern_id)  # 0.68 ‚Üí 0.476
```

Over time, consistently good patterns rise, consistently bad patterns fall.

### Bulk Feedback

For reviewing multiple patterns:

```python
def review_patterns(store, intent="general"):
    """Review all patterns of a given intent."""
    patterns = [p for p in store.get_all_patterns() if p.intent == intent]
    
    print(f"Reviewing {len(patterns)} {intent} patterns:\n")
    
    for i, pattern in enumerate(patterns, 1):
        print(f"{i}. {pattern.response_text}")
        print(f"   Score: {pattern.success_score:.3f}, Used: {pattern.usage_count} times")
        
        feedback = input("   Feedback [+/=/- or skip]: ")
        
        if feedback == "+":
            store.upvote(pattern.fragment_id)
        elif feedback == "-":
            store.downvote(pattern.fragment_id)
```

---

## Updated Summary

1. **BioNN State**: Shared encoder (stateless), isolated pattern learning ‚úì
2. **Git Ignore**: Updated to exclude all user data ‚úì
3. **User Separation**: Complete isolation, easy maintenance ‚úì
4. **Bad Information**: Natural decay + teacher override + pruning tools ‚úì
5. **Corruption Recovery**: Automatic detection and recovery ‚úì
6. **User Data Reset**: Clean slate with backup ‚úì
7. **Manual Feedback**: Upvote/downvote for quality control ‚úì

The architecture is designed for robust multi-user operation with built-in mechanisms for knowledge quality control and error recovery.
