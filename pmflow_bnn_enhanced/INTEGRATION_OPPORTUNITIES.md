# PMFlow Enhancement Integration Opportunities

This document identifies places in the Lilith codebase that could benefit from the new PMFlow retrieval extensions.

## Summary

Found **4 integration opportunities** where manual cosine similarity could be replaced with PMFlow-enhanced retrieval:

1. âœ… **database_fragment_store.py** - Query expansion DONE
2. ðŸŽ¯ **concept_store.py** - Manual similarity â†’ CompositionalRetrieval
3. ðŸŽ¯ **response_composer.py** - Add attention-weighted composition
4. ðŸŽ¯ **concept_store.py** - Consolidation with SemanticNeighborhood

---

## 1. âœ… Query Expansion (COMPLETED)

**File:** `database_fragment_store.py`  
**Line:** 414-559 (retrieve_patterns_hybrid)

**Current State:** âœ… Already integrated QueryExpansionPMField

**What was done:**
```python
# Enhanced retrieve_patterns_hybrid() with query expansion
if use_query_expansion and hasattr(self.encoder, 'pm_field'):
    if not hasattr(self, '_query_expander'):
        self._query_expander = QueryExpansionPMField(
            self.encoder.pm_field, expansion_k=5
        )
    
    expanded_latent, _ = self._query_expander.expand_query(query_latent)
    query_embedding = process_expanded(expanded_latent)
```

**Impact:** Improves synonym matching (ML â†’ machine learning)

---

## 2. ðŸŽ¯ ConceptStore Manual Similarity

**File:** `experiments/retrieval_sanity/poc_compositional/concept_store.py`  
**Lines:** 193, 302-310

**Current Code:**
```python
def retrieve_similar(self, query_embedding, top_k=5, min_similarity=0.60):
    results = []
    for concept in self.concepts.values():
        # Manual cosine similarity calculation
        similarity = self._cosine_similarity(query_embedding, concept.embedding)
        if similarity >= min_similarity:
            results.append((concept, similarity))
    
    # Sort and return top-k
    results.sort(key=lambda x: x[1], reverse=True)
    return results[:top_k]

def _cosine_similarity(self, a: np.ndarray, b: np.ndarray) -> float:
    """Manual cosine calculation"""
    dot_product = np.dot(a, b)
    norm_a = np.linalg.norm(a)
    norm_b = np.linalg.norm(b)
    return dot_product / (norm_a * norm_b) if norm_a > 0 and norm_b > 0 else 0.0
```

**Proposed Enhancement:** Use CompositionalRetrievalPMField

```python
from pmflow_bnn_enhanced import CompositionalRetrievalPMField

class ConceptStore:
    def __init__(self, encoder, storage_path=None):
        self.encoder = encoder
        self.storage_path = storage_path
        self.concepts = {}
        
        # NEW: Compositional retrieval pipeline
        if hasattr(encoder, 'pm_field'):
            self.retrieval = CompositionalRetrievalPMField(
                encoder.pm_field,
                expansion_k=5
            )
    
    def retrieve_similar(
        self, 
        query_embedding, 
        top_k=5, 
        min_similarity=0.60,
        use_expansion=True,
        use_hierarchical=True
    ):
        """Enhanced retrieval using PMFlow extensions."""
        
        if not hasattr(self, 'retrieval'):
            # Fallback to manual similarity
            return self._manual_retrieve_similar(query_embedding, top_k, min_similarity)
        
        # Convert all concept embeddings to tensor
        concept_ids = list(self.concepts.keys())
        concept_embeddings = []
        
        for cid in concept_ids:
            concept = self.concepts[cid]
            if concept.embedding is None:
                concept.embedding = self.encoder.encode(concept.term.split())
            concept_embeddings.append(
                torch.from_numpy(concept.embedding).float()
            )
        
        concept_tensor = torch.stack(concept_embeddings)  # (N, D)
        query_tensor = torch.from_numpy(query_embedding).float().unsqueeze(0)  # (1, D)
        
        # Use compositional retrieval
        results = self.retrieval.retrieve_concepts(
            query_tensor,
            concept_tensor,
            expand_query=use_expansion,
            use_hierarchical=use_hierarchical,
            min_similarity=min_similarity
        )
        
        # Convert back to (concept, score) tuples
        concept_results = []
        for idx, score in results[:top_k]:
            concept = self.concepts[concept_ids[idx]]
            concept.usage_count += 1
            concept_results.append((concept, score))
        
        return concept_results
```

**Benefits:**
- âœ… Query expansion for synonym matching
- âœ… Hierarchical filtering (10x speedup when many concepts)
- âœ… Attention-weighted ranking
- âœ… All embarrassingly parallel

**Impact:** 
- Speed: 10x faster with 100+ concepts
- Quality: Better synonym matching via expansion
- Architecture: Cleaner, uses PMFlow's physics

---

## 3. ðŸŽ¯ ResponseComposer Attention Weighting

**File:** `experiments/retrieval_sanity/pipeline/response_composer.py`  
**Lines:** 165-320 (compose_response method)

**Current Code:**
```python
def compose_response(self, context, user_input="", topk=5, ...):
    # 1. Retrieve patterns
    patterns = self.fragments.retrieve_patterns_hybrid(...)
    
    # 2. Weight by PMFlow activations (current: simple scoring)
    weighted_patterns = self._weight_by_pmflow_activations(patterns, context)
    
    # 3. Filter for coherence
    coherent_patterns = self._filter_coherent_patterns(weighted_patterns, ...)
    
    # 4. Compose final response
    response = self._compose_from_patterns(coherent_patterns, ...)
```

**Proposed Enhancement:** Attention-weighted pattern composition

```python
from pmflow_bnn_enhanced import AttentionWeightedRetrieval

class ResponseComposer:
    def __init__(self, fragment_store, conversation_state, ...):
        self.fragments = fragment_store
        self.state = conversation_state
        
        # NEW: Attention-weighted composition
        if hasattr(fragment_store.encoder, 'pm_field'):
            self.attention_composer = AttentionWeightedRetrieval(
                fragment_store.encoder.pm_field
            )
    
    def compose_response(self, context, user_input="", topk=5, ...):
        # Retrieve candidate patterns
        patterns = self.fragments.retrieve_patterns_hybrid(
            context, topk=topk * 2, ...  # Get more candidates
        )
        
        if not patterns:
            return self._fallback_response(user_input)
        
        # NEW: Attention-weighted re-ranking
        if hasattr(self, 'attention_composer'):
            patterns = self._attention_rerank(patterns, context, topk)
        else:
            # Old path: simple PMFlow weighting
            patterns = self._weight_by_pmflow_activations(patterns, context)[:topk]
        
        # Filter and compose
        coherent_patterns = self._filter_coherent_patterns(patterns, ...)
        response = self._compose_from_patterns(coherent_patterns, ...)
        return response
    
    def _attention_rerank(self, patterns, context, topk):
        """Re-rank patterns using gravitational attention."""
        
        # Encode context as query
        context_tokens = context.lower().split()
        query_emb = self.fragments.encoder.encode(context_tokens)
        query_z = torch.from_numpy(query_emb).float().unsqueeze(0)
        
        # Encode all pattern triggers
        pattern_embeddings = []
        for pattern, _ in patterns:
            trigger_tokens = pattern.trigger_context.lower().split()
            pattern_emb = self.fragments.encoder.encode(trigger_tokens)
            pattern_embeddings.append(
                torch.from_numpy(pattern_emb).float()
            )
        
        pattern_tensor = torch.stack(pattern_embeddings)  # (N, D)
        
        # Get attention-weighted ranking
        indices, weights = self.attention_composer.retrieve_weighted(
            query_z, pattern_tensor, top_k=topk
        )
        
        # Return reranked patterns with new weights
        reranked = []
        for i, idx in enumerate(indices):
            pattern = patterns[idx][0]  # Original pattern
            weight = float(weights[i])
            reranked.append((pattern, weight))
        
        return reranked
```

**Benefits:**
- âœ… Gravitational potential captures "semantic coherence"
- âœ… Better multi-pattern composition (finds complementary patterns)
- âœ… Embarrassingly parallel

**Impact:**
- Quality: More coherent multi-turn responses
- Architecture: Patterns weighted by PMFlow physics, not ad-hoc scoring

---

## 4. ðŸŽ¯ ConceptStore Consolidation

**File:** `experiments/retrieval_sanity/poc_compositional/concept_store.py`  
**Lines:** 232-287 (merge_similar_concepts)

**Current Code:**
```python
def merge_similar_concepts(self, threshold=0.92):
    """Brute force O(NÂ²) consolidation."""
    merged_count = 0
    concepts_to_remove = set()
    
    concept_list = list(self.concepts.values())
    
    for i, concept_a in enumerate(concept_list):
        for concept_b in concept_list[i+1:]:
            # Manual similarity calculation
            similarity = self._cosine_similarity(
                concept_a.embedding, 
                concept_b.embedding
            )
            
            if similarity >= threshold:
                # Merge B into A
                ...
```

**Proposed Enhancement:** SemanticNeighborhood clustering

```python
from pmflow_bnn_enhanced import SemanticNeighborhoodPMField

class ConceptStore:
    def __init__(self, encoder, storage_path=None):
        # ... existing code ...
        
        # NEW: Semantic neighborhood for clustering
        if hasattr(encoder, 'pm_field'):
            self.neighborhood = SemanticNeighborhoodPMField(
                encoder.pm_field
            )
    
    def merge_similar_concepts(self, threshold=0.85):
        """Enhanced consolidation using field signatures."""
        
        if not hasattr(self, 'neighborhood'):
            # Fallback to manual O(NÂ²) merging
            return self._manual_merge(threshold)
        
        merged_count = 0
        concepts_to_remove = set()
        
        # Convert all concepts to tensor
        concept_ids = list(self.concepts.keys())
        concept_embeddings = torch.stack([
            torch.from_numpy(self.concepts[cid].embedding).float()
            for cid in concept_ids
        ])  # (N, D)
        
        # For each concept, find its neighbors using field signatures
        for i, cid in enumerate(concept_ids):
            if cid in concepts_to_remove:
                continue
            
            concept_z = concept_embeddings[i:i+1]  # (1, D)
            
            # Find neighbors based on similar gravitational field signatures
            neighbor_indices, scores = self.neighborhood.find_neighbors(
                concept_z,
                concept_embeddings,
                threshold=threshold
            )
            
            # Merge neighbors into this concept
            for j, score in zip(neighbor_indices, scores):
                if j <= i:  # Skip self and already processed
                    continue
                
                neighbor_cid = concept_ids[j]
                if neighbor_cid in concepts_to_remove:
                    continue
                
                # Merge neighbor into current concept
                print(f"  ðŸ”— Merging '{self.concepts[neighbor_cid].term}' "
                      f"into '{self.concepts[cid].term}' "
                      f"(field similarity: {score:.3f})")
                
                self._merge_concept_data(
                    self.concepts[cid],
                    self.concepts[neighbor_cid]
                )
                
                concepts_to_remove.add(neighbor_cid)
                merged_count += 1
        
        # Remove merged concepts
        for cid in concepts_to_remove:
            del self.concepts[cid]
        
        if merged_count > 0:
            self._save_concepts()
        
        return merged_count
```

**Benefits:**
- âœ… Field signatures capture "similar relational structure"
- âœ… More principled than pure cosine distance
- âœ… Finds concepts that "behave similarly" in PMFlow space
- âœ… Embarrassingly parallel (each query independent)

**Impact:**
- Quality: Better consolidation (finds concepts with similar roles)
- Performance: Still O(NÂ²) but smarter matching
- Architecture: Uses PMFlow's gravitational semantics

---

## Integration Priority

### High Priority (Immediate Value)

1. âœ… **Query expansion in database_fragment_store.py** - DONE
   - Status: Integrated and tested
   - Impact: Synonym matching improvement

### Medium Priority (Production Phase 1)

2. ðŸŽ¯ **ConceptStore compositional retrieval**
   - File: `poc_compositional/concept_store.py`
   - Benefit: 10x speedup + query expansion
   - Effort: ~30 minutes
   - Risk: Low (graceful fallback exists)

3. ðŸŽ¯ **ResponseComposer attention weighting**
   - File: `pipeline/response_composer.py`
   - Benefit: Better multi-pattern composition
   - Effort: ~45 minutes
   - Risk: Low (can keep old weighting as fallback)

### Low Priority (Optimization)

4. ðŸŽ¯ **ConceptStore consolidation**
   - File: `poc_compositional/concept_store.py`
   - Benefit: Smarter merging via field signatures
   - Effort: ~30 minutes
   - Risk: Medium (changes consolidation logic)

---

## Testing Strategy

### For Each Integration:

1. **Create comparison test**
   ```python
   # Test: before vs after enhancement
   results_without = old_method(query, ...)
   results_with = new_method(query, ...)
   
   # Compare:
   # - Quality: Better matches?
   # - Speed: Faster?
   # - Compatibility: No regressions?
   ```

2. **Validate embarrassingly parallel**
   - No hidden state
   - No inter-sample dependencies
   - Batch processing works

3. **Measure impact**
   - Retrieval quality (precision@k)
   - Speed (queries/second)
   - Memory (bytes per concept)

---

## Next Steps

1. âœ… Query expansion - VALIDATED
2. ðŸŽ¯ Integrate ConceptStore compositional retrieval
3. ðŸŽ¯ Test on PoC compositional architecture
4. ðŸŽ¯ Measure quality improvement
5. ðŸŽ¯ Add ResponseComposer attention weighting
6. ðŸŽ¯ Production deployment

---

## Benefits Summary

**Query Expansion (DONE):**
- Synonym matching: "ML" â†’ "machine learning" âœ…
- Graceful fallback âœ…
- Tested and working âœ…

**ConceptStore Enhancements:**
- 10x faster hierarchical retrieval
- Better synonym matching via expansion
- Smarter consolidation via field signatures
- All embarrassingly parallel

**ResponseComposer Enhancements:**
- Physics-based pattern weighting
- Better multi-pattern coherence
- Attention-weighted composition

**Total Integration Time:** ~2-3 hours
**Expected Impact:** Significant quality + speed improvements
**Risk Level:** Low (all have fallbacks)

---

## Architecture Consistency

All enhancements maintain embarrassingly parallel semantics:
- âœ… Stateless (no hidden state between calls)
- âœ… Vectorized (batch operations on GPU)
- âœ… Independent (no inter-sample dependencies)

This preserves PMFlow's core architectural principle while adding powerful retrieval capabilities.
